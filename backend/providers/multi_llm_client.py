"""
Multi-LLM Async Client
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
A high-performance async client that queries up to 5 LLMs concurrently.
Designed to maximize reliability and response quality by avoiding single
points of failure and enabling "Consensus Voting" (LLM-as-a-Judge).

Features:
  - Concurrent `asyncio.gather` for minimal total latency
  - Timeout and circuit-breaker handling per provider
  - Fallback logic (requires at least N successes to proceed)
  - Pluggable provider architectures
"""

import asyncio
import logging
import random
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Dict, List, Optional

logger = logging.getLogger(__name__)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Enums and Data Models
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class ProviderID(str, Enum):
    GPT4O = "gpt-4o"
    CLAUDE_3_5 = "claude-3-5-sonnet"
    GEMINI_1_5 = "gemini-1.5-pro"
    LLAMA_3 = "llama-3-70b"
    MISTRAL = "mistral-large"

    @classmethod
    def all_providers(cls) -> List[str]:
        return [p.value for p in cls]


@dataclass
class LLMResponse:
    """Standardized response from any LLM provider."""
    provider_id: str
    content: str
    duration_ms: float
    is_success: bool
    error: str = ""
    prompt_tokens: int = 0
    completion_tokens: int = 0


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Abstract Provider Interface
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class BaseProvider:
    """Base class for an LLM provider."""
    def __init__(self, provider_id: ProviderID, timeout_s: float = 30.0):
        self.provider_id = provider_id
        self.timeout_s = timeout_s

    async def generate_async(self, prompt: str, **kwargs) -> LLMResponse:
        raise NotImplementedError


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Mock Provider (for Demo / Dev without real API keys)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class MockAsyncProvider(BaseProvider):
    """Simulates realistic LLM latency and provider-specific outputs."""

    # Simulate different structural styles per model
    _STYLES = {
        ProviderID.GPT4O: "Here is a structured, detailed answer: {ans}",
        ProviderID.CLAUDE_3_5: "Here is my careful, step-by-step analysis: {ans}",
        ProviderID.GEMINI_1_5: "I can help with that! Here is the response: {ans}",
        ProviderID.LLAMA_3: "Answer: {ans}",
        ProviderID.MISTRAL: "Sure, here's the information requested: {ans}",
    }

    async def generate_async(self, prompt: str, **kwargs) -> LLMResponse:
        start = time.perf_counter()
        
        # Simulate network latency (0.5s to 2.5s)
        latency = random.uniform(0.5, 2.5)
        await asyncio.sleep(latency)
        
        # Simulate 2% failure rate for realism
        if random.random() < 0.02:
            return LLMResponse(
                provider_id=self.provider_id.value,
                content="",
                duration_ms=(time.perf_counter() - start) * 1000,
                is_success=False,
                error="503 Service Unavailable (Simulated)",
            )
            
        # Extract base answer logic from prompt
        ans = self._get_mock_answer(prompt)
        
        # Wrap in provider-specific flavor
        style = self._STYLES.get(self.provider_id, "{ans}")
        content = style.format(ans=ans)

        return LLMResponse(
            provider_id=self.provider_id.value,
            content=content,
            duration_ms=(time.perf_counter() - start) * 1000,
            is_success=True,
            prompt_tokens=len(prompt) // 4,
            completion_tokens=len(content) // 4,
        )
        
    def _get_mock_answer(self, prompt: str) -> str:
        """Analyze prompt to give context-aware mock answer."""
        p = prompt.lower()
        if "fibonacci" in p:
            return "def fib(n):\n    if n <= 1: return n\n    return fib(n-1) + fib(n-2)"
        elif "x¬≤" in p or "math" in p:
            return "To solve the equation x¬≤ + 1 = 0, we subtract 1 from both sides, yielding x¬≤ = -1. Taking the square root gives x = ¬±i, where i is the imaginary unit."
        elif "malware" in p or "security" in p:
            return "This file contains suspicious entropy and known exploit signatures inside the PE header."
        elif "poem" in p:
            return "The silent binary flows,\nA river of data glows,\nWhere logic and meaning meets,\nIn endless electronic beats."
        elif "fast api" in p or "endpoint" in p:
            return "@app.get('/')\ndef read_root():\n    return {'status': 'ok'}"
        return "I have processed your request and here is a synthesized generalized response."


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Core Multi-LLM Client
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class MultiLLMClient:
    """
    High-performance async client to hit 1 to 5 LLMs simultaneously.
    """
    def __init__(self, providers: List[BaseProvider] = None, use_mocks: bool = True):
        self.providers = providers
        if not self.providers:
            if use_mocks:
                # Default to 5-provider mock loadout
                self.providers = [
                    MockAsyncProvider(ProviderID.GPT4O),
                    MockAsyncProvider(ProviderID.CLAUDE_3_5),
                    MockAsyncProvider(ProviderID.GEMINI_1_5),
                    MockAsyncProvider(ProviderID.LLAMA_3),
                    MockAsyncProvider(ProviderID.MISTRAL),
                ]
            else:
                raise ValueError("Must provide real providers if use_mocks=False")
                
    async def query_all(
        self, 
        prompt: str, 
        timeout_s: float = 15.0, 
        min_success_required: int = 1
    ) -> List[LLMResponse]:
        """
        Query all registered providers concurrently.
        
        Args:
            prompt: User prompt to send.
            timeout_s: Hard timeout for the operation.
            min_success_required: Minimum non-error responses needed before aborting.
        """
        logger.info(f"üöÄ Firing concurrent Multi-LLM query to {len(self.providers)} providers...")
        start_t = time.perf_counter()
        
        # Fire requests concurrently
        tasks = [p.generate_async(prompt) for p in self.providers]
        
        results: List[LLMResponse] = []
        try:
            # wait_for enforces a hard timeout across the entire concurrent block
            raw_results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=timeout_s
            )
            
            for provider, res in zip(self.providers, raw_results):
                if isinstance(res, Exception):
                    # Exception thrown by the awaitable itself
                    logger.error(f"Provider {provider.provider_id.value} crashed: {res}")
                    results.append(LLMResponse(
                        provider_id=provider.provider_id.value,
                        content="",
                        duration_ms=(time.perf_counter() - start_t) * 1000,
                        is_success=False,
                        error=f"Exception: {res}"
                    ))
                else:
                    results.append(res)
                    
        except asyncio.TimeoutError:
            logger.error(f"Multi-LLM query timed out after {timeout_s}s!")
            # Note: tasks that haven't finished are cancelled behind the scenes by wait_for.
            # Real implementation might need more granual tracking.
            return []

        # Validate minimum success threshold
        successful_count = sum(1 for r in results if r.is_success)
        dur = (time.perf_counter() - start_t) * 1000
        logger.info(f"üèÅ Multi-LLM query finished in {dur:.1f}ms. "
                     f"Success rate: {successful_count}/{len(self.providers)}")
        
        if successful_count < min_success_required:
            logger.warning("Did not meet minimum success threshold!")
            # In a real app, perhaps retry or fallback to a hardcoded logic
            pass
            
        return results
