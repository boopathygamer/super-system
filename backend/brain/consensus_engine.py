"""
Consensus Engine â€” LLM-as-a-Judge Output Ranking
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Evaluates multiple concurrent outputs from different LLM providers,
scores them across rigorous dimensions (Accuracy, Reasoning, Code),
and promotes the mathematically highest-scoring response.

Features:
  - Multi-dimensional scoring matrix
  - Penalty for hallucinations or missing constraints
  - Fast-path fallback
"""

import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from providers.multi_llm_client import LLMResponse, ProviderID

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Data Models
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class ScoreMatrix:
    """Multi-dimensional score for an LLM response."""
    provider_id: str
    accuracy: float = 0.0          # 0.0 - 1.0
    reasoning_depth: float = 0.0   # 0.0 - 1.0
    code_quality: float = 0.0      # 0.0 - 1.0 (if applicable)
    formatting: float = 0.0        # 0.0 - 1.0
    
    # Negative signals
    hallucination_penalty: float = 0.0
    verbosity_penalty: float = 0.0
    
    @property
    def final_score(self) -> float:
        """Weighted sum minus penalties."""
        base = (
            (self.accuracy * 0.40) +
            (self.reasoning_depth * 0.30) +
            (self.code_quality * 0.20) +
            (self.formatting * 0.10)
        )
        return max(0.0, base - self.hallucination_penalty - self.verbosity_penalty)


@dataclass
class ConsensusVerdict:
    """The final decision from the Consensus Engine."""
    winner_provider: str
    winning_response: str
    winning_score: float
    durations_ms: Dict[str, float] = field(default_factory=dict)
    scores: Dict[str, ScoreMatrix] = field(default_factory=dict)
    synthesis_applied: bool = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# The Consensus Judge
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ConsensusEngine:
    """
    Acts as the final judge over a panel of N LLM responses.
    """
    
    def __init__(self, judge_model: str = "fast-judge-model"):
        self.judge_model = judge_model
        
    def evaluate_and_rank(self, original_prompt: str, responses: List[LLMResponse]) -> ConsensusVerdict:
        """
        Rank a set of responses and pick the absolute best one.
        
        Args:
            original_prompt: The initial prompt that triggered the generation.
            responses: The raw responses from `MultiLLMClient`.
        """
        logger.info(f"âš–ï¸ Consensus Engine: Evaluating {len(responses)} raw responses...")
        
        if not responses:
            return ConsensusVerdict(
                winner_provider="none",
                winning_response="ERROR: No responses received from any provider.",
                winning_score=0.0
            )

        valid_responses = [r for r in responses if r.is_success and r.content.strip()]
        if not valid_responses:
            # Fallback if all errored out
            return ConsensusVerdict(
                winner_provider=responses[0].provider_id,
                winning_response=f"Fallback Error: {responses[0].error}",
                winning_score=0.0
            )

        scores_by_provider: Dict[str, ScoreMatrix] = {}
        durations_by_provider: Dict[str, float] = {}
        
        # â”€â”€ LLM-as-a-Judge Simulation â”€â”€
        # In a real environment, we would send a prompt to self.judge_model asking it:
        # "Given this prompt: {original_prompt}, rank these N responses..."
        # Due to API latency, we simulate the evaluation locally using heuristics for the mock.
        
        for res in valid_responses:
            durations_by_provider[res.provider_id] = res.duration_ms
            matrix = self._heuristic_judge(original_prompt, res.content, res.provider_id)
            scores_by_provider[res.provider_id] = matrix
            logger.debug(f"   Provider [{res.provider_id}] Score: {matrix.final_score:.2f}")

        # Pick the winner
        winner_id = max(scores_by_provider.keys(), key=lambda pid: scores_by_provider[pid].final_score)
        winning_score = scores_by_provider[winner_id].final_score
        
        # Find the actual content
        winning_content = next(r.content for r in valid_responses if r.provider_id == winner_id)
        
        logger.info(f"ðŸ† Consensus Engine picked [{winner_id}] (Score: {winning_score:.2f})")
        
        return ConsensusVerdict(
            winner_provider=winner_id,
            winning_response=winning_content,
            winning_score=winning_score,
            durations_ms=durations_by_provider,
            scores=scores_by_provider,
        )
        
    def _heuristic_judge(self, prompt: str, content: str, provider_id: str) -> ScoreMatrix:
        """
        Simulate an LLM judge evaluating a response.
        """
        import re
        content_lower = content.lower()
        prompt_lower = prompt.lower()
        
        accuracy = 0.8  # Base assumption
        reasoning = 0.6
        code_quality = 0.5
        formatting = 0.7
        penalty = 0.0
        
        # Format checks
        if "```" in content:
            formatting += 0.2
            if "def " in content or "class " in content:
                code_quality = 0.9
        
        # Specific provider biases (simulating different model strengths)
        if provider_id == ProviderID.GPT4O.value:
            reasoning += 0.3
            formatting += 0.1
        elif provider_id == ProviderID.CLAUDE_3_5.value:
            accuracy += 0.15
            code_quality += 0.3
        elif provider_id == ProviderID.LLAMA_3.value:
            reasoning += 0.2
            
        # Check against constraints
        if "fibonacci" in prompt_lower and "fib" not in content_lower:
            accuracy -= 0.5
            penalty += 0.4
            
        # Basic length checks
        if len(content) < 10:
            penalty += 0.5
            
        return ScoreMatrix(
            provider_id=provider_id,
            accuracy=min(1.0, accuracy),
            reasoning_depth=min(1.0, reasoning),
            code_quality=min(1.0, code_quality),
            formatting=min(1.0, formatting),
            hallucination_penalty=penalty
        )
